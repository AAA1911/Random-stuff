{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "text_features.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DnLV1HUefFtW"
      },
      "source": [
        "# Text Features In CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWbrfteXTh-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "b64517ac-852c-4938-c18c-6635320ca079"
      },
      "source": [
        "\n",
        "train_df, test_df = rotten_tomatoes()\n",
        "\n",
        "train_df.head(2)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>synopsis</th>\n",
              "      <th>rating_MPAA</th>\n",
              "      <th>genre</th>\n",
              "      <th>director</th>\n",
              "      <th>writer</th>\n",
              "      <th>theater_date</th>\n",
              "      <th>dvd_date</th>\n",
              "      <th>box_office</th>\n",
              "      <th>runtime</th>\n",
              "      <th>studio</th>\n",
              "      <th>dvd_date_int</th>\n",
              "      <th>theater_date_int</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>fresh</th>\n",
              "      <th>critic</th>\n",
              "      <th>top_critic</th>\n",
              "      <th>publisher</th>\n",
              "      <th>date</th>\n",
              "      <th>date_int</th>\n",
              "      <th>rating_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>830.0</td>\n",
              "      <td>A gay New Yorker stages a marriage of convenie...</td>\n",
              "      <td>R</td>\n",
              "      <td>Art House and International | Comedy | Drama |...</td>\n",
              "      <td>Ang Lee</td>\n",
              "      <td>Ang Lee | James Schamus | Neil Peng</td>\n",
              "      <td>1993-08-04</td>\n",
              "      <td>2004-06-15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>111.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20040615</td>\n",
              "      <td>19930804</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>fresh</td>\n",
              "      <td>Carol Cling</td>\n",
              "      <td>0</td>\n",
              "      <td>Las Vegas Review-Journal</td>\n",
              "      <td>2004-04-16</td>\n",
              "      <td>20040416.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1161.0</td>\n",
              "      <td>Screenwriter Nimrod Antal makes an impressive ...</td>\n",
              "      <td>R</td>\n",
              "      <td>Action and Adventure | Art House and Internati...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2005-04-01</td>\n",
              "      <td>2005-08-30</td>\n",
              "      <td>116783.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>ThinkFilm Inc.</td>\n",
              "      <td>20050830</td>\n",
              "      <td>20050401</td>\n",
              "      <td>One very long, dark ride.</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>rotten</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>E! Online</td>\n",
              "      <td>2005-04-22</td>\n",
              "      <td>20050422.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... rating_10\n",
              "0   830.0  ...       8.0\n",
              "1  1161.0  ...       6.0\n",
              "\n",
              "[2 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n0wW6VT2TmhR",
        "colab": {}
      },
      "source": [
        "auxiliary_columns = ['id', 'theater_date', 'dvd_date', 'rating', 'date']\n",
        "cat_features = ['rating_MPAA', 'studio', 'fresh', 'critic', 'top_critic', 'publisher']\n",
        "text_features = ['synopsis', 'genre', 'director', 'writer', 'review']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t-stj0CUTmhV",
        "colab": {}
      },
      "source": [
        "def fill_na(df, features):\n",
        "    for feature in features:\n",
        "        df[feature].fillna('', inplace=True)\n",
        "\n",
        "def preprocess_data_part(data_part):\n",
        "    data_part = data_part.drop(auxiliary_columns, axis=1)\n",
        "\n",
        "    fill_na(data_part, cat_features)\n",
        "    fill_na(data_part, text_features)\n",
        "\n",
        "    X = data_part.drop(['rating_10'], axis=1)\n",
        "    y = data_part['rating_10']\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_data_part(train_df)\n",
        "X_test, y_test = preprocess_data_part(test_df)\n",
        "\n",
        "X_train_no_text = X_train.drop(text_features, axis=1)\n",
        "X_test_no_text = X_test.drop(text_features, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTch2k92Th7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "b7dbe377-c07e-435d-9c39-19c679f70f85"
      },
      "source": [
        "stop_words = set(('be', 'is', 'are', 'the', 'an', 'of', 'and', 'in'))\n",
        "\n",
        "def filter_stop_words(tokens):\n",
        "    return list(filter(lambda x: x not in stop_words, tokens))\n",
        "    \n",
        "tokenized_text_no_stop = [filter_stop_words(tokens) for tokens in tokenized_text]\n",
        "tokenized_text_no_stop"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['cats', 'so', 'cute'],\n",
              " ['mouse', 'scare'],\n",
              " ['cat', 'defeated', 'mouse'],\n",
              " ['cute', 'mice', 'gather', 'army'],\n",
              " ['army', 'mice', 'defeated', 'cat'],\n",
              " ['cat', 'offers', 'peace'],\n",
              " ['cat', 'scared'],\n",
              " ['cat', 'mouse', 'live', 'peace']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB2iYc44Th46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fill_na(df, features):\n",
        "    for feature in features:\n",
        "        df[feature].fillna('', inplace=True)\n",
        "\n",
        "def preprocess_data_part(data_part):\n",
        "    data_part = data_part.drop(auxiliary_columns, axis=1)\n",
        "\n",
        "    fill_na(data_part, cat_features)\n",
        "    fill_na(data_part, text_features)\n",
        "\n",
        "    X = data_part.drop(['rating_10'], axis=1)\n",
        "    y = data_part['rating_10']\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_data_part(train_df)\n",
        "X_test, y_test = preprocess_data_part(test_df)\n",
        "\n",
        "X_train_no_text = X_train.drop(text_features, axis=1)\n",
        "X_test_no_text = X_test.drop(text_features, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tuos7W0qTh3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "def fit_model(train_pool, validation_pool, **kwargs):\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=3000,\n",
        "        learning_rate=0.05,\n",
        "        depth=4,\n",
        "        eval_metric='Accuracy',\n",
        "        task_type='GPU',\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return model.fit(\n",
        "        train_pool,\n",
        "        eval_set=validation_pool,\n",
        "        verbose=100,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruA5SyxqThxG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "c7f5c03d-9e5b-4f7f-8970-e7fe9a972eb7"
      },
      "source": [
        "train_pool = Pool(\n",
        "    X_train, y_train, \n",
        "    cat_features=cat_features,\n",
        "    text_features=text_features,\n",
        ")\n",
        "\n",
        "validation_pool = Pool(\n",
        "    X_test, y_test, \n",
        "    cat_features=cat_features,\n",
        "    text_features=text_features,\n",
        ")\n",
        "\n",
        "print('Train dataset shape: {}\\n'.format(train_pool.shape))\n",
        "\n",
        "model = fit_model(train_pool, validation_pool)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset shape: (32712, 16)\n",
            "\n",
            "0:\tlearn: 0.3804109\ttest: 0.3913681\tbest: 0.3913681 (0)\ttotal: 98.2ms\tremaining: 4m 54s\n",
            "100:\tlearn: 0.4376987\ttest: 0.4495660\tbest: 0.4501773 (96)\ttotal: 6.61s\tremaining: 3m 9s\n",
            "200:\tlearn: 0.4478173\ttest: 0.4571464\tbest: 0.4572686 (193)\ttotal: 12.2s\tremaining: 2m 49s\n",
            "300:\tlearn: 0.4547567\ttest: 0.4608143\tbest: 0.4621592 (279)\ttotal: 17.5s\tremaining: 2m 36s\n",
            "400:\tlearn: 0.4607483\ttest: 0.4617924\tbest: 0.4626482 (394)\ttotal: 22.8s\tremaining: 2m 27s\n",
            "500:\tlearn: 0.4658229\ttest: 0.4627705\tbest: 0.4637486 (483)\ttotal: 28.2s\tremaining: 2m 20s\n",
            "600:\tlearn: 0.4700110\ttest: 0.4647267\tbest: 0.4650935 (576)\ttotal: 33.3s\tremaining: 2m 12s\n",
            "700:\tlearn: 0.4734654\ttest: 0.4654603\tbest: 0.4680279 (654)\ttotal: 38.4s\tremaining: 2m 5s\n",
            "800:\tlearn: 0.4764612\ttest: 0.4674166\tbest: 0.4680279 (654)\ttotal: 43.5s\tremaining: 1m 59s\n",
            "900:\tlearn: 0.4792737\ttest: 0.4666830\tbest: 0.4680279 (654)\ttotal: 48.6s\tremaining: 1m 53s\n",
            "1000:\tlearn: 0.4820555\ttest: 0.4653381\tbest: 0.4680279 (654)\ttotal: 53.7s\tremaining: 1m 47s\n",
            "1100:\tlearn: 0.4856322\ttest: 0.4638709\tbest: 0.4680279 (654)\ttotal: 58.9s\tremaining: 1m 41s\n",
            "1200:\tlearn: 0.4883835\ttest: 0.4632596\tbest: 0.4680279 (654)\ttotal: 1m 3s\tremaining: 1m 35s\n",
            "1300:\tlearn: 0.4914404\ttest: 0.4637486\tbest: 0.4680279 (654)\ttotal: 1m 9s\tremaining: 1m 30s\n",
            "1400:\tlearn: 0.4937026\ttest: 0.4641154\tbest: 0.4680279 (654)\ttotal: 1m 14s\tremaining: 1m 24s\n",
            "1500:\tlearn: 0.4965150\ttest: 0.4637486\tbest: 0.4680279 (654)\ttotal: 1m 19s\tremaining: 1m 19s\n",
            "1600:\tlearn: 0.4994192\ttest: 0.4641154\tbest: 0.4680279 (654)\ttotal: 1m 24s\tremaining: 1m 13s\n",
            "1700:\tlearn: 0.5016202\ttest: 0.4653381\tbest: 0.4680279 (654)\ttotal: 1m 29s\tremaining: 1m 8s\n",
            "1800:\tlearn: 0.5045549\ttest: 0.4647267\tbest: 0.4680279 (654)\ttotal: 1m 34s\tremaining: 1m 2s\n",
            "1900:\tlearn: 0.5069088\ttest: 0.4675388\tbest: 0.4680279 (654)\ttotal: 1m 39s\tremaining: 57.6s\n",
            "2000:\tlearn: 0.5095989\ttest: 0.4672943\tbest: 0.4683947 (1906)\ttotal: 1m 44s\tremaining: 52.3s\n",
            "2100:\tlearn: 0.5120445\ttest: 0.4669275\tbest: 0.4683947 (1906)\ttotal: 1m 49s\tremaining: 47s\n",
            "2200:\tlearn: 0.5148569\ttest: 0.4665607\tbest: 0.4683947 (1906)\ttotal: 1m 54s\tremaining: 41.7s\n",
            "2300:\tlearn: 0.5169663\ttest: 0.4675388\tbest: 0.4683947 (1906)\ttotal: 2m\tremaining: 36.5s\n",
            "2400:\tlearn: 0.5185865\ttest: 0.4676611\tbest: 0.4691283 (2334)\ttotal: 2m 5s\tremaining: 31.2s\n",
            "2500:\tlearn: 0.5212155\ttest: 0.4658271\tbest: 0.4691283 (2334)\ttotal: 2m 10s\tremaining: 26s\n",
            "2600:\tlearn: 0.5235388\ttest: 0.4655826\tbest: 0.4691283 (2334)\ttotal: 2m 15s\tremaining: 20.7s\n",
            "2700:\tlearn: 0.5264735\ttest: 0.4654603\tbest: 0.4691283 (2334)\ttotal: 2m 20s\tremaining: 15.5s\n",
            "2800:\tlearn: 0.5285216\ttest: 0.4658271\tbest: 0.4691283 (2334)\ttotal: 2m 25s\tremaining: 10.3s\n",
            "2900:\tlearn: 0.5310895\ttest: 0.4660716\tbest: 0.4691283 (2334)\ttotal: 2m 30s\tremaining: 5.14s\n",
            "2999:\tlearn: 0.5328014\ttest: 0.4652158\tbest: 0.4691283 (2334)\ttotal: 2m 35s\tremaining: 0us\n",
            "bestTest = 0.4691282553\n",
            "bestIteration = 2334\n",
            "Shrink model to first 2335 iterations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPqQ-g9fThvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_score_diff(first_model, second_model):\n",
        "    first_accuracy = first_model.best_score_['validation']['Accuracy']\n",
        "    second_accuracy = second_model.best_score_['validation']['Accuracy']\n",
        "\n",
        "    gap = (second_accuracy - first_accuracy) / first_accuracy * 100\n",
        "\n",
        "    print('{} vs {} ({:+.2f}%)'.format(first_accuracy, second_accuracy, gap))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iYp72J1Thtv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19393dce-42bd-4e8d-f1e8-10191e289196"
      },
      "source": [
        "print_score_diff(model_no_text, model)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4565350287321188 vs 0.4691282552879325 (+2.76%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVaK2OtDThsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWz1rR_lThqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBfZUwwXThoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BBWQXONThmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_v6_5yFThkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4cHLNgJThie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqCEnvU_ThfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY0fuazSThYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTdq7i7xThWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R004N1xhThTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0UAHpnD8fFtZ"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catboost/tutorials/blob/master/events/2020_06_04_catboost_tutorial/text_features.ipynb)\n",
        "\n",
        "**Set GPU as hardware accelerator**\n",
        "\n",
        "First of all, you need to select GPU as hardware accelerator. There are two simple steps to do so:\n",
        "Step 1. Navigate to **Runtime** menu and select **Change runtime type**\n",
        "Step 2. Choose **GPU** as hardware accelerator.\n",
        "That's all!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9FM0IRyi8NOw"
      },
      "source": [
        "Let's install CatBoost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TpJdgt63fSOv",
        "outputId": "30dc8dd6-1c47-472c-bfc6-41aa13aa24bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/aa/e61819d04ef2bbee778bf4b3a748db1f3ad23512377e43ecfdc3211437a0/catboost-0.23.2-cp36-none-manylinux1_x86_64.whl (64.8MB)\n",
            "\u001b[K     |████████████████████████████████| 64.8MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.23.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "viF18QJqfFtd"
      },
      "source": [
        "In this tutorial we will use dataset **Rotten Tomatoes Movie Reviews** from [Kaggle](https://www.kaggle.com) competition for our experiments. Data can be downloaded [here](https://www.kaggle.com/rpnuser8182/rotten-tomatoes/data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MNC1tP0UfFtd",
        "outputId": "28be2462-2a92-4cbc-ce65-02e88e16ce0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "import catboost\n",
        "print(catboost.__version__)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.23.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OkexL1k7fFti"
      },
      "source": [
        "## Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m11CtnPEfFtj",
        "outputId": "cc5e5ab3-5636-4dd7-8b98-473ded7eba73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "from catboost.datasets import rotten_tomatoes\n",
        "\n",
        "train_df, test_df = rotten_tomatoes()\n",
        "\n",
        "train_df.head(2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>synopsis</th>\n",
              "      <th>rating_MPAA</th>\n",
              "      <th>genre</th>\n",
              "      <th>director</th>\n",
              "      <th>writer</th>\n",
              "      <th>theater_date</th>\n",
              "      <th>dvd_date</th>\n",
              "      <th>box_office</th>\n",
              "      <th>runtime</th>\n",
              "      <th>studio</th>\n",
              "      <th>dvd_date_int</th>\n",
              "      <th>theater_date_int</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>fresh</th>\n",
              "      <th>critic</th>\n",
              "      <th>top_critic</th>\n",
              "      <th>publisher</th>\n",
              "      <th>date</th>\n",
              "      <th>date_int</th>\n",
              "      <th>rating_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>830.0</td>\n",
              "      <td>A gay New Yorker stages a marriage of convenie...</td>\n",
              "      <td>R</td>\n",
              "      <td>Art House and International | Comedy | Drama |...</td>\n",
              "      <td>Ang Lee</td>\n",
              "      <td>Ang Lee | James Schamus | Neil Peng</td>\n",
              "      <td>1993-08-04</td>\n",
              "      <td>2004-06-15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>111.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20040615</td>\n",
              "      <td>19930804</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>fresh</td>\n",
              "      <td>Carol Cling</td>\n",
              "      <td>0</td>\n",
              "      <td>Las Vegas Review-Journal</td>\n",
              "      <td>2004-04-16</td>\n",
              "      <td>20040416.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1161.0</td>\n",
              "      <td>Screenwriter Nimrod Antal makes an impressive ...</td>\n",
              "      <td>R</td>\n",
              "      <td>Action and Adventure | Art House and Internati...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2005-04-01</td>\n",
              "      <td>2005-08-30</td>\n",
              "      <td>116783.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>ThinkFilm Inc.</td>\n",
              "      <td>20050830</td>\n",
              "      <td>20050401</td>\n",
              "      <td>One very long, dark ride.</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>rotten</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>E! Online</td>\n",
              "      <td>2005-04-22</td>\n",
              "      <td>20050422.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... rating_10\n",
              "0   830.0  ...       8.0\n",
              "1  1161.0  ...       6.0\n",
              "\n",
              "[2 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8IeOEa1gfFtm"
      },
      "source": [
        "### Features description \n",
        "\n",
        "|Id | Feature name      |  Description                                                                                 |\n",
        "|---|-------------------|----------------------------------------------------------------------------------------------|\n",
        "| 1 | ``id``            |  unique movie id                                                                             |\n",
        "| 2 | ``synopsis``      |  brief summary of the major points of a movie                                                |\n",
        "| 3 | ``rating_MPAA``   |  film rating by MPAA rating system                                                           |\n",
        "| 4 | ``genre``         |  list of genres that are suitable for this film (e.g. Action, Adventure, Comedy,...          |\n",
        "| 5 | ``director``      |  list of persons who direct the making of a film                                             |\n",
        "| 6 | ``writer``        |  list of persons who write a screenplay                                                      |\n",
        "| 7 | ``theater_date``  |  the date when film was first shown to the public in cinema (string)                         |\n",
        "| 8 | ``dvd_date``      |  the date when film was released on DVD (string)                                             |\n",
        "| 9 | ``box_office``    |  the amount of money raised by ticket sales (revenue)                                        |\n",
        "| 10 | ``runtime``      |  film duration in minutes                                                                    |\n",
        "| 11 | ``studio``       |  is a major entertainment company or motion picture company (20th Century Fox, Sony Pictures)|\n",
        "| 12 | ``dvd_date_int`` |  the date when film was released on DVD (converted to integer)                               |\n",
        "| 13 | ``theater_date_int`` |  the date when film was first shown to the public in cinema (converted to integer)       |\n",
        "| 14 | ``review``       |  review of a movie, that was written by a critic                                             |\n",
        "| 15 | ``rating``       |  float rating from 0 to 1 of the film according to the Rotten tomatoes web site              |\n",
        "| 16 | ``fresh``        |  freshness of review - fresh or rotten                                                       |\n",
        "| 17 | ``critic``       |  name of reviewer                                                                            |\n",
        "| 18 | ``top_critic``   |  binary feature, is reviewer a top critic or not                                             |\n",
        "| 19 | ``publisher``    |  journal or website where the review was published                                           |\n",
        "| 20 | ``date``         |  the date when critic publish review (string)                                                |\n",
        "| 21 | ``date_int``     |  the date when critic publish review (converted to integer)                                  |\n",
        "| 22 | ``rating_10``    |  integer rating from 0 to 10 of the film according to the critic                             |\n",
        "\n",
        "We mark as **auxiliary** columnns 'id' and 'rating', because they can be the reason of overfitting, 'theater_date','dvd_date','date' because we convert them into integers.\n",
        "\n",
        "We mark as **text** features 'synopsis' because it is short *text* description of a film, 'genre' because it is combination of categories (we know that strings have structure where words define categories), for example 'Action | Comedy | Adventure', 'director' and 'writer' features are included to the text features by the same reason, 'review' becuase it is a *text* summary of critic opinion.\n",
        "\n",
        "We mark as **categorical** features 'rating_MPAA', 'studio', 'fresh', 'critic', 'top_critic' and 'publisher' because they can not be splitted into the group of categorical features and feature values can not be compared.\n",
        "\n",
        "The other columns considered as **numeric**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wJRY9YyVfFtl"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qy_gcs7qfFtn",
        "colab": {}
      },
      "source": [
        "auxiliary_columns = ['id', 'theater_date', 'dvd_date', 'rating', 'date']\n",
        "cat_features = ['rating_MPAA', 'studio', 'fresh', 'critic', 'top_critic', 'publisher']\n",
        "text_features = ['synopsis', 'genre', 'director', 'writer', 'review']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WkV114UDfFtp",
        "colab": {}
      },
      "source": [
        "def fill_na(df, features):\n",
        "    for feature in features:\n",
        "        df[feature].fillna('', inplace=True)\n",
        "\n",
        "def preprocess_data_part(data_part):\n",
        "    data_part = data_part.drop(auxiliary_columns, axis=1)\n",
        "\n",
        "    fill_na(data_part, cat_features)\n",
        "    fill_na(data_part, text_features)\n",
        "\n",
        "    X = data_part.drop(['rating_10'], axis=1)\n",
        "    y = data_part['rating_10']\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_data_part(train_df)\n",
        "X_test, y_test = preprocess_data_part(test_df)\n",
        "\n",
        "X_train_no_text = X_train.drop(text_features, axis=1)\n",
        "X_test_no_text = X_test.drop(text_features, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OfkxzEZXfFtr",
        "outputId": "4ad1250e-037d-43c7-a6b2-73ecd22efd51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "X_train_no_text.head(2)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating_MPAA</th>\n",
              "      <th>box_office</th>\n",
              "      <th>runtime</th>\n",
              "      <th>studio</th>\n",
              "      <th>dvd_date_int</th>\n",
              "      <th>theater_date_int</th>\n",
              "      <th>fresh</th>\n",
              "      <th>critic</th>\n",
              "      <th>top_critic</th>\n",
              "      <th>publisher</th>\n",
              "      <th>date_int</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>R</td>\n",
              "      <td>NaN</td>\n",
              "      <td>111.0</td>\n",
              "      <td></td>\n",
              "      <td>20040615</td>\n",
              "      <td>19930804</td>\n",
              "      <td>fresh</td>\n",
              "      <td>Carol Cling</td>\n",
              "      <td>0</td>\n",
              "      <td>Las Vegas Review-Journal</td>\n",
              "      <td>20040416.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>R</td>\n",
              "      <td>116783.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>ThinkFilm Inc.</td>\n",
              "      <td>20050830</td>\n",
              "      <td>20050401</td>\n",
              "      <td>rotten</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>E! Online</td>\n",
              "      <td>20050422.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  rating_MPAA  box_office  ...                 publisher    date_int\n",
              "0           R         NaN  ...  Las Vegas Review-Journal  20040416.0\n",
              "1           R    116783.0  ...                 E! Online  20050422.0\n",
              "\n",
              "[2 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssW0cX4wRuQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "bff6b8b7-fe57-4110-aebb-0b1292d24111"
      },
      "source": [
        "stop_words = set(('be', 'is', 'are', 'the', 'an', 'of', 'and', 'in'))\n",
        "\n",
        "def filter_stop_words(tokens):\n",
        "    return list(filter(lambda x: x not in stop_words, tokens))\n",
        "    \n",
        "tokenized_text_no_stop = [filter_stop_words(tokens) for tokens in tokenized_text]\n",
        "tokenized_text_no_stop"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['cats', 'so', 'cute'],\n",
              " ['mouse', 'scare'],\n",
              " ['cat', 'defeated', 'mouse'],\n",
              " ['cute', 'mice', 'gather', 'army'],\n",
              " ['army', 'mice', 'defeated', 'cat'],\n",
              " ['cat', 'offers', 'peace'],\n",
              " ['cat', 'scared'],\n",
              " ['cat', 'mouse', 'live', 'peace']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBQjRpxRuOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsuxIUydRuJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQtPWsBTRuEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CTq7w0U9fFtt",
        "outputId": "d6ebb883-f6cf-408a-c6a3-d92c0027b91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from catboost import Pool\n",
        "\n",
        "train_pool_no_text = Pool(\n",
        "    X_train_no_text, y_train, \n",
        "    cat_features=cat_features, \n",
        ")\n",
        "\n",
        "validation_pool_no_text = Pool(\n",
        "    X_test_no_text, y_test, \n",
        "    cat_features=cat_features, \n",
        ")\n",
        "\n",
        "print('Train dataset shape: {}\\n'.format(train_pool_no_text.shape))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset shape: (32712, 11)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VTi3eN58fFt6",
        "outputId": "3cedbf20-9809-46c2-d691-d0416a150f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        }
      },
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "def fit_model(train_pool, validation_pool, **kwargs):\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=3000,\n",
        "        learning_rate=0.05,\n",
        "        depth=4,\n",
        "        eval_metric='Accuracy',\n",
        "        task_type='GPU',\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return model.fit(\n",
        "        train_pool,\n",
        "        eval_set=validation_pool,\n",
        "        verbose=100,\n",
        "    )\n",
        "\n",
        "model_no_text = fit_model(train_pool_no_text, validation_pool_no_text)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.3804109\ttest: 0.3913681\tbest: 0.3913681 (0)\ttotal: 26.4ms\tremaining: 1m 19s\n",
            "100:\tlearn: 0.4177366\ttest: 0.4247463\tbest: 0.4258467 (99)\ttotal: 2.36s\tremaining: 1m 7s\n",
            "200:\tlearn: 0.4305148\ttest: 0.4334271\tbest: 0.4334271 (200)\ttotal: 4.65s\tremaining: 1m 4s\n",
            "300:\tlearn: 0.4378210\ttest: 0.4372173\tbest: 0.4374618 (290)\ttotal: 6.94s\tremaining: 1m 2s\n",
            "400:\tlearn: 0.4443935\ttest: 0.4384399\tbest: 0.4399071 (345)\ttotal: 9.2s\tremaining: 59.7s\n",
            "500:\tlearn: 0.4505380\ttest: 0.4394180\tbest: 0.4402739 (469)\ttotal: 11.4s\tremaining: 57.1s\n",
            "600:\tlearn: 0.4551846\ttest: 0.4434527\tbest: 0.4440641 (597)\ttotal: 13.7s\tremaining: 54.7s\n",
            "700:\tlearn: 0.4590670\ttest: 0.4445531\tbest: 0.4449199 (628)\ttotal: 15.9s\tremaining: 52.1s\n",
            "800:\tlearn: 0.4637136\ttest: 0.4451644\tbest: 0.4452867 (715)\ttotal: 18.1s\tremaining: 49.8s\n",
            "900:\tlearn: 0.4673820\ttest: 0.4460203\tbest: 0.4480988 (860)\ttotal: 20.4s\tremaining: 47.5s\n",
            "1000:\tlearn: 0.4729457\ttest: 0.4485878\tbest: 0.4488324 (985)\ttotal: 22.7s\tremaining: 45.3s\n",
            "1100:\tlearn: 0.4770726\ttest: 0.4494437\tbest: 0.4500550 (1062)\ttotal: 24.9s\tremaining: 42.9s\n",
            "1200:\tlearn: 0.4817192\ttest: 0.4499328\tbest: 0.4501773 (1196)\ttotal: 27.1s\tremaining: 40.6s\n",
            "1300:\tlearn: 0.4844094\ttest: 0.4495660\tbest: 0.4517667 (1261)\ttotal: 29.4s\tremaining: 38.4s\n",
            "1400:\tlearn: 0.4882000\ttest: 0.4523780\tbest: 0.4526226 (1385)\ttotal: 31.7s\tremaining: 36.2s\n",
            "1500:\tlearn: 0.4919296\ttest: 0.4527448\tbest: 0.4527448 (1500)\ttotal: 34s\tremaining: 34s\n",
            "1600:\tlearn: 0.4955062\ttest: 0.4526226\tbest: 0.4527448 (1500)\ttotal: 36.3s\tremaining: 31.7s\n",
            "1700:\tlearn: 0.4984409\ttest: 0.4520112\tbest: 0.4533562 (1614)\ttotal: 38.6s\tremaining: 29.4s\n",
            "1800:\tlearn: 0.5018036\ttest: 0.4528671\tbest: 0.4536007 (1794)\ttotal: 40.8s\tremaining: 27.2s\n",
            "1900:\tlearn: 0.5054414\ttest: 0.4540897\tbest: 0.4545788 (1832)\ttotal: 43s\tremaining: 24.9s\n",
            "2000:\tlearn: 0.5081927\ttest: 0.4543343\tbest: 0.4545788 (1832)\ttotal: 45.2s\tremaining: 22.6s\n",
            "2100:\tlearn: 0.5117694\ttest: 0.4534784\tbest: 0.4548233 (2086)\ttotal: 47.5s\tremaining: 20.3s\n",
            "2200:\tlearn: 0.5145207\ttest: 0.4539675\tbest: 0.4551901 (2146)\ttotal: 49.7s\tremaining: 18.1s\n",
            "2300:\tlearn: 0.5176082\ttest: 0.4543343\tbest: 0.4551901 (2146)\ttotal: 52s\tremaining: 15.8s\n",
            "2400:\tlearn: 0.5201455\ttest: 0.4547011\tbest: 0.4554346 (2363)\ttotal: 54.2s\tremaining: 13.5s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-16368a20607c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel_no_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool_no_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_pool_no_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-16368a20607c>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(train_pool, validation_pool, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   4113\u001b[0m         self._fit(X, y, cat_features, text_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[1;32m   4114\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n\u001b[0m\u001b[1;32m   4116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   1741\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m                 \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m                 \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"init_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m             )\n\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QhF2RAAhfFuJ"
      },
      "source": [
        "# Text Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Aw0M5trY8Dmg",
        "outputId": "4d68394f-cd82-4d29-87a5-8924d74e500b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "train_pool = Pool(\n",
        "    X_train, y_train, \n",
        "    cat_features=cat_features,\n",
        "    text_features=text_features,\n",
        ")\n",
        "\n",
        "validation_pool = Pool(\n",
        "    X_test, y_test, \n",
        "    cat_features=cat_features,\n",
        "    text_features=text_features,\n",
        ")\n",
        "\n",
        "print('Train dataset shape: {}\\n'.format(train_pool.shape))\n",
        "\n",
        "model = fit_model(train_pool, validation_pool)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset shape: (32712, 16)\n",
            "\n",
            "0:\tlearn: 0.3858217\ttest: 0.3980927\tbest: 0.3980927 (0)\ttotal: 184ms\tremaining: 3m 3s\n",
            "100:\tlearn: 0.4529225\ttest: 0.4584913\tbest: 0.4584913 (99)\ttotal: 12.5s\tremaining: 1m 51s\n",
            "200:\tlearn: 0.4671986\ttest: 0.4649713\tbest: 0.4649713 (200)\ttotal: 22.3s\tremaining: 1m 28s\n",
            "300:\tlearn: 0.4773783\ttest: 0.4671720\tbest: 0.4671720 (300)\ttotal: 31.8s\tremaining: 1m 13s\n",
            "400:\tlearn: 0.4850514\ttest: 0.4670498\tbest: 0.4686392 (325)\ttotal: 41.6s\tremaining: 1m 2s\n",
            "500:\tlearn: 0.4941306\ttest: 0.4675388\tbest: 0.4691283 (432)\ttotal: 51.5s\tremaining: 51.2s\n",
            "600:\tlearn: 0.5018342\ttest: 0.4680279\tbest: 0.4694950 (584)\ttotal: 1m\tremaining: 40.4s\n",
            "700:\tlearn: 0.5083761\ttest: 0.4691283\tbest: 0.4701064 (694)\ttotal: 1m 10s\tremaining: 29.9s\n",
            "800:\tlearn: 0.5156823\ttest: 0.4686392\tbest: 0.4702286 (731)\ttotal: 1m 19s\tremaining: 19.7s\n",
            "900:\tlearn: 0.5233859\ttest: 0.4694950\tbest: 0.4702286 (731)\ttotal: 1m 28s\tremaining: 9.76s\n",
            "999:\tlearn: 0.5297444\ttest: 0.4691283\tbest: 0.4702286 (731)\ttotal: 1m 37s\tremaining: 0us\n",
            "bestTest = 0.4702286343\n",
            "bestIteration = 731\n",
            "Shrink model to first 732 iterations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HsuS5qKnfFuQ",
        "colab": {}
      },
      "source": [
        "def print_score_diff(first_model, second_model):\n",
        "    first_accuracy = first_model.best_score_['validation']['Accuracy']\n",
        "    second_accuracy = second_model.best_score_['validation']['Accuracy']\n",
        "\n",
        "    gap = (second_accuracy - first_accuracy) / first_accuracy * 100\n",
        "\n",
        "    print('{} vs {} ({:+.2f}%)'.format(first_accuracy, second_accuracy, gap))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O-3uDpJafFuS",
        "outputId": "ed47017c-1f98-43d7-8e65-826f9d5d9298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print_score_diff(model_no_text, model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4565350287321188 vs 0.47022863430737255 (+3.00%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ym-fEV-mfFuU"
      },
      "source": [
        "<span style=\"color:red\">Note!</span>\n",
        "\n",
        "1. Text features also cannot contain NaN values, so we converted them into strings manually.\n",
        "2. The training may be performed only with classification losses and targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IiHpTGfbfFuV"
      },
      "source": [
        "## How it works?\n",
        "\n",
        "1. **Text Tokenization**\n",
        "2. **Dictionary Creation**\n",
        "3. **Feature Calculation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MszSnbqH8NR3"
      },
      "source": [
        "## Text Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mOBGuexjb8tr"
      },
      "source": [
        "Usually we get our text as a sequence of Unicode symbols. So, if the task isn't a DNA classification we don't need such granularity, moreover, we need to extract more complicated entities, e.g. words. The process of extraction tokens -- words, numbers, punctuation symbols or special symbols which defines emoji from a sequence is called **tokenization**.<br>\n",
        "\n",
        "Tokenization is the first part of text preprocessing in CatBoost and performed as a simple splitting a sequence on a string pattern (e.g. space)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trPDH7fKTAj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tkBFYCwTAgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ3d6nWTAaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP5T86LTTAZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niAKSZDgTAYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFicHWNZTAVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NAeELULufFuV",
        "colab": {}
      },
      "source": [
        "text_small = [\n",
        "    \"Cats are so cute :)\",\n",
        "    \"Mouse scare...\",\n",
        "    \"The cat defeated the mouse\",\n",
        "    \"Cute: Mice gather an army!\",\n",
        "    \"Army of mice defeated the cat :(\",\n",
        "    \"Cat offers peace\",\n",
        "    \"Cat is scared :(\",\n",
        "    \"Cat and mouse live in peace :)\"\n",
        "]\n",
        "\n",
        "target_small = [1, 0, 1, 1, 0, 1, 0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E21CQ8ocfFuX",
        "outputId": "28c3194f-45df-463b-b0e4-28d07fbc0ada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "from catboost.text_processing import Tokenizer\n",
        "\n",
        "simple_tokenizer = Tokenizer()\n",
        "\n",
        "def tokenize_texts(texts):\n",
        "    return [simple_tokenizer.tokenize(text) for text in texts]\n",
        "\n",
        "simple_tokenized_text = tokenize_texts(text_small)\n",
        "simple_tokenized_text"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Cats', 'are', 'so', 'cute', ':)'],\n",
              " ['Mouse', 'scare...'],\n",
              " ['The', 'cat', 'defeated', 'the', 'mouse'],\n",
              " ['Cute:', 'Mice', 'gather', 'an', 'army!'],\n",
              " ['Army', 'of', 'mice', 'defeated', 'the', 'cat', ':('],\n",
              " ['Cat', 'offers', 'peace'],\n",
              " ['Cat', 'is', 'scared', ':('],\n",
              " ['Cat', 'and', 'mouse', 'live', 'in', 'peace', ':)']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ChZQ5cpJfFuZ"
      },
      "source": [
        "### More preprocessing!\n",
        "\n",
        "Lets take a closer look on the tokenization result of small text example -- the tokens contains a lot of mistakes:\n",
        "\n",
        "1. They are glued with punctuation 'Cute:', 'army!', 'skare...'.\n",
        "2. The words 'Cat' and 'cat', 'Mice' and 'mice' seems to have same meaning, perhaps they should be the same tokens.\n",
        "3. The same problem with tokens 'are'/'is' -- they are inflected forms of same token 'be'.\n",
        "\n",
        "**Punctuation handling**, **lowercasing**, and **lemmatization** processes help to overcome these problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qaoTjEmR8NSM"
      },
      "source": [
        "### Punctuation handling and lowercasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6cPpYpmtfFuZ",
        "outputId": "2f688b17-8889-4035-9fff-f4fff17cad48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "tokenizer = Tokenizer(\n",
        "    lowercasing=True,\n",
        "    separator_type='BySense',\n",
        "    token_types=['Word', 'Number']\n",
        ")\n",
        "\n",
        "tokenized_text = [tokenizer.tokenize(text) for text in text_small]\n",
        "tokenized_text"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['cats', 'are', 'so', 'cute'],\n",
              " ['mouse', 'scare'],\n",
              " ['the', 'cat', 'defeated', 'the', 'mouse'],\n",
              " ['cute', 'mice', 'gather', 'an', 'army'],\n",
              " ['army', 'of', 'mice', 'defeated', 'the', 'cat'],\n",
              " ['cat', 'offers', 'peace'],\n",
              " ['cat', 'is', 'scared'],\n",
              " ['cat', 'and', 'mouse', 'live', 'in', 'peace']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JDhBkZzJfFua"
      },
      "source": [
        "### Removing stop words\n",
        "\n",
        "**Stop words** - the words that are considered to be uninformative in this task, e.g. function words such as *the, is, at, which, on*.\n",
        "Usually stop words are removed during text preprocessing to reduce the amount of information that is considered for further algorithms.\n",
        "Stop words are collected manually (in dictionary form) or automatically, for example taking the most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d1MYzKgTfFub",
        "outputId": "e6e84fed-e2d2-4993-b2a7-993e7bf18057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "stop_words = set(('be', 'is', 'are', 'the', 'an', 'of', 'and', 'in'))\n",
        "\n",
        "def filter_stop_words(tokens):\n",
        "    return list(filter(lambda x: x not in stop_words, tokens))\n",
        "    \n",
        "tokenized_text_no_stop = [filter_stop_words(tokens) for tokens in tokenized_text]\n",
        "tokenized_text_no_stop"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['cats', 'so', 'cute'],\n",
              " ['mouse', 'scare'],\n",
              " ['cat', 'defeated', 'mouse'],\n",
              " ['cute', 'mice', 'gather', 'army'],\n",
              " ['army', 'mice', 'defeated', 'cat'],\n",
              " ['cat', 'offers', 'peace'],\n",
              " ['cat', 'scared'],\n",
              " ['cat', 'mouse', 'live', 'peace']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vxofPVc1fFuc"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "Lemma (Wikipedia) -- is the canonical form, dictionary form, or citation form of a set of words.<br>\n",
        "For example, the lemma \"go\" represents the inflected forms \"go\", \"goes\", \"going\", \"went\", and \"gone\".<br>\n",
        "The process of convertation word to its lemma called **lemmatization**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HWrijpMGfFud",
        "outputId": "5f0f101a-04d7-41c0-f4f0-ac64aec7f95b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk_data_path = os.path.join(os.path.dirname(nltk.__file__), 'nltk_data')\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "nltk.download('wordnet', nltk_data_path)\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens_nltk(tokens):\n",
        "    return list(map(lambda t: lemmatizer.lemmatize(t), tokens))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /usr/local/lib/python3.6/dist-\n",
            "[nltk_data]     packages/nltk/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XfyhV9ONfFuf",
        "outputId": "ee9350bb-f564-4aaf-d2ad-ce9576b6997f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "text_small_lemmatized_nltk = [lemmatize_tokens_nltk(tokens) for tokens in tokenized_text_no_stop]\n",
        "text_small_lemmatized_nltk"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['cat', 'so', 'cute'],\n",
              " ['mouse', 'scare'],\n",
              " ['cat', 'defeated', 'mouse'],\n",
              " ['cute', 'mouse', 'gather', 'army'],\n",
              " ['army', 'mouse', 'defeated', 'cat'],\n",
              " ['cat', 'offer', 'peace'],\n",
              " ['cat', 'scared'],\n",
              " ['cat', 'mouse', 'live', 'peace']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y63KVna4fFui"
      },
      "source": [
        "Now words with same meaning represented by the same token, tokens are not glued with punctuation.\n",
        "\n",
        "<span style=\"color:red\">Be carefull.</span> You should verify for your own task:<br>\n",
        "Is it realy necessary to remove punctuation, lowercasing sentences or performing a lemmatization and/or by word tokenization?<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFWoSX-kfFui"
      },
      "source": [
        "### Let's check up accuracy with new text preprocessing\n",
        "\n",
        "Since CatBoost doesn't perform spacing punctuation, lowercasing letters and lemmatization, we need to preprocess text manually and then pass it to learning algorithm.\n",
        "\n",
        "Since the natural text features is only synopsis and review, we will preprocess only them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZHL3x7NwfFuj",
        "outputId": "6c5c55b8-45e9-4c74-b677-d837fd2e3715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "def preprocess_data(X):\n",
        "    X_preprocessed = X.copy()\n",
        "    for feature in ['synopsis', 'review']:\n",
        "        X_preprocessed[feature] = X[feature].apply(lambda x: ' '.join(lemmatize_tokens_nltk(tokenizer.tokenize(x))))\n",
        "    return X_preprocessed\n",
        "\n",
        "X_preprocessed_train = preprocess_data(X_train)\n",
        "X_preprocessed_test = preprocess_data(X_test)\n",
        "\n",
        "train_processed_pool = Pool(\n",
        "    X_preprocessed_train, y_train, \n",
        "    cat_features=cat_features,\n",
        "    text_features=text_features,\n",
        ")\n",
        "\n",
        "validation_processed_pool = Pool(\n",
        "    X_preprocessed_test, y_test, \n",
        "    cat_features=cat_features,\n",
        "    text_features=text_features,\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 27.2 s, sys: 405 µs, total: 27.2 s\n",
            "Wall time: 27.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0jJJSrFJfFuk",
        "outputId": "f94cc295-0f1d-4b7d-dc74-4d94d48878a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "model_on_processed_data = fit_model(train_processed_pool, validation_processed_pool)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.3858217\ttest: 0.3980927\tbest: 0.3980927 (0)\ttotal: 180ms\tremaining: 2m 59s\n",
            "100:\tlearn: 0.4542064\ttest: 0.4589803\tbest: 0.4589803 (100)\ttotal: 12.7s\tremaining: 1m 53s\n",
            "200:\tlearn: 0.4693079\ttest: 0.4663162\tbest: 0.4674166 (192)\ttotal: 22.8s\tremaining: 1m 30s\n",
            "300:\tlearn: 0.4810773\ttest: 0.4721849\tbest: 0.4726739 (289)\ttotal: 32.5s\tremaining: 1m 15s\n",
            "400:\tlearn: 0.4899120\ttest: 0.4763419\tbest: 0.4763419 (400)\ttotal: 42.4s\tremaining: 1m 3s\n",
            "500:\tlearn: 0.4982881\ttest: 0.4747524\tbest: 0.4763419 (400)\ttotal: 51.9s\tremaining: 51.7s\n",
            "600:\tlearn: 0.5054414\ttest: 0.4752415\tbest: 0.4767086 (583)\ttotal: 1m 1s\tremaining: 40.8s\n",
            "700:\tlearn: 0.5132979\ttest: 0.4749969\tbest: 0.4767086 (583)\ttotal: 1m 11s\tremaining: 30.3s\n",
            "800:\tlearn: 0.5199621\ttest: 0.4771977\tbest: 0.4771977 (800)\ttotal: 1m 20s\tremaining: 20s\n",
            "900:\tlearn: 0.5278185\ttest: 0.4765864\tbest: 0.4775645 (801)\ttotal: 1m 30s\tremaining: 9.9s\n",
            "999:\tlearn: 0.5356444\ttest: 0.4756083\tbest: 0.4775645 (801)\ttotal: 1m 39s\tremaining: 0us\n",
            "bestTest = 0.4775644944\n",
            "bestIteration = 801\n",
            "Shrink model to first 802 iterations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbyROF95SNws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qORCSn-USNvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxBRqEXpSNta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDneg5I5SNn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AXDdPAgyfFum",
        "outputId": "89567395-9bd9-41bf-db92-dc0b3f14d198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print_score_diff(model, model_on_processed_data)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47022863430737255 vs 0.47756449443697274 (+1.56%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CJr7fXN7fFun"
      },
      "source": [
        "## Dictionary Creation\n",
        "\n",
        "After the first stage, preprocessing of text and tokenization, the second stage starts. The second stage uses the prepared text to select a set of units, which will be used for building new numerical features.\n",
        "\n",
        "A set of selected units is called dictionary. It might contain words, word bigramms, or character n-gramms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D6H1MXf9fFuo",
        "colab": {}
      },
      "source": [
        "from catboost.text_processing import Dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rn402k78fFuq",
        "colab": {}
      },
      "source": [
        "dictionary = Dictionary(occurence_lower_bound=0, max_dictionary_size=10)\n",
        "\n",
        "dictionary.fit(text_small_lemmatized_nltk);\n",
        "#dictionary.fit(text_small, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KJr0UBzOfFur",
        "colab": {}
      },
      "source": [
        "dictionary.save('dictionary.tsv')\n",
        "!cat dictionary.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U1wLb5MX8NTY"
      },
      "source": [
        "## Feature Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYzNqXgcfFut"
      },
      "source": [
        "### Convertation into fixed size vectors\n",
        "\n",
        "The majority of classic ML algorithms are computing and performing predictions on a fixed number of features $F$.<br>\n",
        "That means that learning set $X = \\{x_i\\}$ contains vectors $x_i = (a_0, a_1, ..., a_F)$ where $F$ is constant.\n",
        "\n",
        "Since text object $x$ is not a fixed length vector, we need to perform preprocessing of the origin set $D$.<br>\n",
        "One of the simplest text to vector encoding technique is **Bag of words (BoW)**.\n",
        "\n",
        "### Bag of words algorithm\n",
        "\n",
        "The algorithm takes in a dictionary and a text.<br>\n",
        "During the algorithm text $x = (a_0, a_1, ..., a_k)$ converted into vector $\\tilde x = (b_0, b_1, ..., b_F)$,<br> where $b_i$ is 0/1 (depending on whether there is a word with id=$i$ from dictionary into text $x$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Ea944JbfFuu",
        "outputId": "ea2d1dc7-db4f-4546-ccdc-e96d6e851c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "text_small_lemmatized_nltk"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['cat', 'so', 'cute'],\n",
              " ['mouse', 'scare'],\n",
              " ['cat', 'defeated', 'mouse'],\n",
              " ['cute', 'mouse', 'gather', 'army'],\n",
              " ['army', 'mouse', 'defeated', 'cat'],\n",
              " ['cat', 'offer', 'peace'],\n",
              " ['cat', 'scared'],\n",
              " ['cat', 'mouse', 'live', 'peace']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bRm5Cf5qkzlJ",
        "outputId": "807a25aa-0ddf-4f59-ccb4-a3e278082869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dictionary.apply([text_small_lemmatized_nltk[0]])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ga0AfpT8fFuv",
        "outputId": "4f6bb6a8-2975-4b30-f51a-fa199515ba67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "def bag_of_words(tokenized_text, dictionary):\n",
        "    features = np.zeros((len(tokenized_text), dictionary.size))\n",
        "    for i, tokenized_sentence in enumerate(tokenized_text):\n",
        "        indices = np.array(dictionary.apply([tokenized_sentence])[0])\n",
        "        features[i, indices] = 1\n",
        "    return features\n",
        "\n",
        "bow_features = bag_of_words(text_small_lemmatized_nltk, dictionary)\n",
        "bow_features"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [1., 1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 1., 1., 0., 0., 1., 0., 0., 0.],\n",
              "       [1., 1., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 1., 0., 0., 0., 1., 0., 1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhr-EyPyfFuy",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def fit_linear_model(X, c):\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, c)\n",
        "    return model\n",
        "\n",
        "def fit_naive_bayes(X, c):\n",
        "    clf = MultinomialNB()\n",
        "    if isinstance(X, csr_matrix):\n",
        "        X.eliminate_zeros()\n",
        "    clf.fit(X, c)\n",
        "    return clf\n",
        "\n",
        "def evaluate_model_logloss(model, X, y):\n",
        "    y_pred = model.predict_proba(X)[:,1]\n",
        "    metric = log_loss(y, y_pred)\n",
        "    print('Logloss: ' + str(metric))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GekNCx5ofFuz",
        "outputId": "13c7c151-09fd-487f-dba4-da3cc131ab4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "def evaluate_models(X, y):\n",
        "    linear_model = fit_linear_model(bow_features, target_small)\n",
        "    naive_bayes = fit_naive_bayes(bow_features, target_small)\n",
        "        \n",
        "    print('Linear model')\n",
        "    evaluate_model_logloss(linear_model, X, y)\n",
        "    print('Naive bayes')\n",
        "    evaluate_model_logloss(naive_bayes, X, y)\n",
        "    print('Comparing to constant prediction')\n",
        "    logloss_constant_prediction = log_loss(y, np.ones(shape=(len(text_small), 2)) * 0.5)\n",
        "    print('Logloss: ' + str(logloss_constant_prediction))\n",
        "    \n",
        "evaluate_models(bow_features, target_small)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear model\n",
            "Logloss: 0.5003949004139002\n",
            "Naive bayes\n",
            "Logloss: 0.4528488772318392\n",
            "Comparing to constant prediction\n",
            "Logloss: 0.6931471805599453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uFsAWNE9fFu2",
        "outputId": "e02d5f6c-434a-41e1-e2ca-51ae6ecbbb78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "dictionary = Dictionary(occurence_lower_bound=0)\n",
        "dictionary.fit(text_small_lemmatized_nltk)\n",
        "\n",
        "bow_features = bag_of_words(text_small_lemmatized_nltk, dictionary)\n",
        "evaluate_models(bow_features, target_small)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear model\n",
            "Logloss: 0.46515559462967115\n",
            "Naive bayes\n",
            "Logloss: 0.3680393546716464\n",
            "Comparing to constant prediction\n",
            "Logloss: 0.6931471805599453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A-4J_mcSfAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "452dacc9-7627-4feb-b515-e85d5f122826"
      },
      "source": [
        "bow_features.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMYpAZn7Se-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_linear_model(X, c):\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, c)\n",
        "    return model\n",
        "\n",
        "def fit_naive_bayes(X, c):\n",
        "    clf = MultinomialNB()\n",
        "    if isinstance(X, csr_matrix):\n",
        "        X.eliminate_zeros()\n",
        "    clf.fit(X, c)\n",
        "    return clf\n",
        "\n",
        "def evaluate_model_logloss(model, X, y):\n",
        "    y_pred = model.predict_proba(X)[:,1]\n",
        "    metric = log_loss(y, y_pred)\n",
        "    print('Logloss: ' + str(metric))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK9wQ3B6Se89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfypsK-wSe6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK9YdLrMSe4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHc0wRMPSe0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YRaf3qASex1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yvjUACB_fFu6"
      },
      "source": [
        "### Looking at sequences of letters / words\n",
        "\n",
        "Let's look at the example: texts 'The cat defeated the mouse' and 'Army of mice defeated the cat :('<br>\n",
        "Simplifying it we have three tokens in each sentence 'cat defeat mouse' and 'mouse defeat cat'.<br>\n",
        "After applying BoW we get two equal vectors with the opposite meaning:\n",
        "\n",
        "| cat | mouse | defeat |\n",
        "|-----|-------|--------|\n",
        "| 1   | 1     | 1      |\n",
        "| 1   | 1     | 1      |\n",
        "\n",
        "How to distinguish them?\n",
        "Lets add sequences of words as a single tokens into our dictionary:\n",
        "\n",
        "| cat | mouse | defeat | cat_defeat | mouse_defeat | defeat_cat | defeat_mouse |\n",
        "|-----|-------|--------|------------|--------------|------------|--------------|\n",
        "| 1   | 1     | 1      | 1          | 0            | 0          | 1            |\n",
        "| 1   | 1     | 1      | 0          | 1            | 1          | 0            |\n",
        "\n",
        "**N-gram** is a continguous sequence of $n$ items from a given sample of text or speech (Wikipedia).<br>\n",
        "In example above Bi-gram (Bigram) = 2-gram of words.\n",
        "\n",
        "Ngrams help to add into vectors more information about text structure, moreover there are n-grams has no meanings in separation, for example, 'Mickey Mouse company'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WU6iWFPZClrf",
        "colab": {}
      },
      "source": [
        "dictionary = Dictionary(occurence_lower_bound=0, gram_order=2)\n",
        "dictionary.fit(text_small_lemmatized_nltk)\n",
        "\n",
        "dictionary.save('dictionary.tsv')\n",
        "!cat dictionary.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ypPTi_XXfFu7",
        "colab": {}
      },
      "source": [
        "bow_features = bag_of_words(text_small_lemmatized_nltk, dictionary)\n",
        "evaluate_models(bow_features, target_small)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1uLlIfJHodEL"
      },
      "source": [
        "### Unigram + Bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XaRC74kNfFu8",
        "outputId": "b0dfa1d4-0205-422c-c396-e71b0cf8f9f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "dictionary1 = Dictionary(occurence_lower_bound=0)\n",
        "dictionary1.fit(text_small_lemmatized_nltk)\n",
        "\n",
        "bow_features1 = bag_of_words(text_small_lemmatized_nltk, dictionary1)\n",
        "\n",
        "dictionary2 = Dictionary(occurence_lower_bound=0, gram_order=2)\n",
        "dictionary2.fit(text_small_lemmatized_nltk)\n",
        "\n",
        "bow_features2 = bag_of_words(text_small_lemmatized_nltk, dictionary2)\n",
        "\n",
        "bow_features = np.concatenate((bow_features1, bow_features2), axis=1)\n",
        "evaluate_models(bow_features, target_small)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear model\n",
            "Logloss: 0.3226184546657689\n",
            "Naive bayes\n",
            "Logloss: 0.13103685350656918\n",
            "Comparing to constant prediction\n",
            "Logloss: 0.6931471805599453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oFR_rMfH8NT_"
      },
      "source": [
        "## CatBoost Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8xoFAOiz8NT_"
      },
      "source": [
        "Parameter names:\n",
        "\n",
        "1. **Text Tokenization** - `tokenizers`\n",
        "2. **Dictionary Creation** - `dictionaries`\n",
        "3. **Feature Calculation** - `feature_calcers`\n",
        "\n",
        "\\* More complex configuration with `text_processing` parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wntt3XrYgkhf"
      },
      "source": [
        "### `tokenizers`\n",
        "\n",
        "Tokenizers used to preprocess Text type feature columns before creating the dictionary.\n",
        "\n",
        "[Documentation](https://catboost.ai/docs/references/tokenizer_options.html).\n",
        "\n",
        "```\n",
        "tokenizers = [{\n",
        "\t'tokenizerId': 'Space',\n",
        "\t'delimiter': ' ',\n",
        "\t'separator_type': 'ByDelimiter',\n",
        "},{\n",
        "\t'tokenizerId': 'Sense',\n",
        "\t'separator_type': 'BySense',\n",
        "}]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aKqHyav7fFu-"
      },
      "source": [
        "### `dictionaries`\n",
        "\n",
        "Dictionaries used to preprocess Text type feature columns.\n",
        "\n",
        "[Documentation](https://catboost.ai/docs/references/dictionaries_options.html).\n",
        "\n",
        "```\n",
        "dictionaries = [{\n",
        "\t'dictionaryId': 'Unigram',\n",
        "\t'max_dictionary_size': '50000',\n",
        "\t'gram_count': '1',\n",
        "},{\n",
        "\t'dictionaryId': 'Bigram',\n",
        "\t'max_dictionary_size': '50000',\n",
        "\t'gram_count': '2',\n",
        "},{\n",
        "\t'dictionaryId': 'Trigram',\n",
        "\t'token_level_type': 'Letter',\n",
        "\t'max_dictionary_size': '50000',\n",
        "\t'gram_count': '3',\n",
        "}]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JT6I_LN98NUC"
      },
      "source": [
        "### `feature_calcers`\n",
        "\n",
        "Feature calcers used to calculate new features based on preprocessed Text type feature columns.\n",
        "\n",
        "1. **`BoW`**<br>\n",
        "Bag of words: 0/1 features (text sample has or not token_id).<br>\n",
        "Number of produced numeric features = dictionary size.<br>\n",
        "Parameters: `top_tokens_count` - maximum number of tokens that will be used for vectorization in bag of words, the most frequent $n$ tokens are taken (**highly affect both on CPU ang GPU RAM usage**).\n",
        "\n",
        "2. **`NaiveBayes`**<br>\n",
        "NaiveBayes: [Multinomial naive bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes) model. As many new features as classes are added. This feature is calculated by analogy with counters in CatBoost by permutation ([estimation of CTRs](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html)). In other words, a random permutation is made and then we go from top to bottom on the dataset and calculate the probability of its belonging to this class for each object.\n",
        "\n",
        "3. **`BM25`**<br>\n",
        "[BM25](https://en.wikipedia.org/wiki/Okapi_BM25). As many new features as classes are added. The idea is the same as in Naive Bayes, but for each class we calculate not the conditional probability, but a certain relevance, which is similar to tf-idf, where the tokens instead of the words and the classes instead of the documents (or rather, the unification of all texts of this class). Only the tf multiplier in BM25 is replaced with another multiplier, which gives an advantage to classes that contain rare tokens.\n",
        "\n",
        "```\n",
        "feature_calcers = [\n",
        "\t'BoW:top_tokens_count=1000',\n",
        "\t'NaiveBayes',\n",
        "\t'BM25',\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "02lH5f1PgpYM"
      },
      "source": [
        "### `text_processing`\n",
        "\n",
        "```\n",
        "text_processing = {\n",
        "    \"tokenizers\" : [{\n",
        "        \"tokenizer_id\" : \"Space\",\n",
        "        \"separator_type\" : \"ByDelimiter\",\n",
        "        \"delimiter\" : \" \"\n",
        "    }],\n",
        "\n",
        "    \"dictionaries\" : [{\n",
        "        \"dictionary_id\" : \"BiGram\",\n",
        "        \"max_dictionary_size\" : \"50000\",\n",
        "        \"occurrence_lower_bound\" : \"3\",\n",
        "        \"gram_order\" : \"2\"\n",
        "    }, {\n",
        "        \"dictionary_id\" : \"Word\",\n",
        "        \"max_dictionary_size\" : \"50000\",\n",
        "        \"occurrence_lower_bound\" : \"3\",\n",
        "        \"gram_order\" : \"1\"\n",
        "    }],\n",
        "\n",
        "    \"feature_processing\" : {\n",
        "        \"default\" : [{\n",
        "            \"dictionaries_names\" : [\"BiGram\", \"Word\"],\n",
        "            \"feature_calcers\" : [\"BoW\"],\n",
        "            \"tokenizers_names\" : [\"Space\"]\n",
        "        }, {\n",
        "            \"dictionaries_names\" : [\"Word\"],\n",
        "            \"feature_calcers\" : [\"NaiveBayes\"],\n",
        "            \"tokenizers_names\" : [\"Space\"]\n",
        "        }],\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-HOhMr-ffFu_",
        "outputId": "b8f7befa-8a47-47d2-ae23-24c357750bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "model_on_processed_data_2 = fit_model(\n",
        "    train_processed_pool,\n",
        "    validation_processed_pool,\n",
        "    text_processing = {\n",
        "        \"tokenizers\" : [{\n",
        "            \"tokenizer_id\" : \"Space\",\n",
        "            \"separator_type\" : \"ByDelimiter\",\n",
        "            \"delimiter\" : \" \"\n",
        "        }],\n",
        "    \n",
        "        \"dictionaries\" : [{\n",
        "            \"dictionary_id\" : \"BiGram\",\n",
        "            \"max_dictionary_size\" : \"50000\",\n",
        "            \"occurrence_lower_bound\" : \"3\",\n",
        "            \"gram_order\" : \"2\"\n",
        "        }, {\n",
        "            \"dictionary_id\" : \"Word\",\n",
        "            \"max_dictionary_size\" : \"50000\",\n",
        "            \"occurrence_lower_bound\" : \"3\",\n",
        "            \"gram_order\" : \"1\"\n",
        "        }],\n",
        "    \n",
        "        \"feature_processing\" : {\n",
        "            \"default\" : [{\n",
        "                \"dictionaries_names\" : [\"BiGram\", \"Word\"],\n",
        "                \"feature_calcers\" : [\"BoW\"],\n",
        "                \"tokenizers_names\" : [\"Space\"]\n",
        "            }, {\n",
        "                \"dictionaries_names\" : [\"Word\"],\n",
        "                \"feature_calcers\" : [\"NaiveBayes\"],\n",
        "                \"tokenizers_names\" : [\"Space\"]\n",
        "            }],\n",
        "        }\n",
        "    }\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.3858217\ttest: 0.3980927\tbest: 0.3980927 (0)\ttotal: 180ms\tremaining: 2m 59s\n",
            "100:\tlearn: 0.4542064\ttest: 0.4589803\tbest: 0.4589803 (100)\ttotal: 12.6s\tremaining: 1m 52s\n",
            "200:\tlearn: 0.4693079\ttest: 0.4663162\tbest: 0.4674166 (192)\ttotal: 22.6s\tremaining: 1m 29s\n",
            "300:\tlearn: 0.4810773\ttest: 0.4721849\tbest: 0.4726739 (289)\ttotal: 32.3s\tremaining: 1m 15s\n",
            "400:\tlearn: 0.4899120\ttest: 0.4763419\tbest: 0.4763419 (400)\ttotal: 42.2s\tremaining: 1m 3s\n",
            "500:\tlearn: 0.4982881\ttest: 0.4747524\tbest: 0.4763419 (400)\ttotal: 51.7s\tremaining: 51.5s\n",
            "600:\tlearn: 0.5054414\ttest: 0.4752415\tbest: 0.4767086 (583)\ttotal: 1m 1s\tremaining: 40.7s\n",
            "700:\tlearn: 0.5132979\ttest: 0.4749969\tbest: 0.4767086 (583)\ttotal: 1m 10s\tremaining: 30.2s\n",
            "800:\tlearn: 0.5199621\ttest: 0.4771977\tbest: 0.4771977 (800)\ttotal: 1m 20s\tremaining: 20s\n",
            "900:\tlearn: 0.5278185\ttest: 0.4765864\tbest: 0.4775645 (801)\ttotal: 1m 29s\tremaining: 9.85s\n",
            "999:\tlearn: 0.5356444\ttest: 0.4756083\tbest: 0.4775645 (801)\ttotal: 1m 39s\tremaining: 0us\n",
            "bestTest = 0.4775644944\n",
            "bestIteration = 801\n",
            "Shrink model to first 802 iterations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFJRD9RofFvC",
        "outputId": "79153a22-b429-45d7-ac9b-5f03ef182491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print_score_diff(model_no_text, model_on_processed_data_2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4565350287321188 vs 0.47756449443697274 (+4.61%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xlo77dzufFvE"
      },
      "source": [
        "# Summary: Text features in CatBoost\n",
        "\n",
        "### The algorithm:\n",
        "1. Input text is loaded as a usual column. ``text_column: [string]``.\n",
        "2. Each text sample is tokenized via splitting by space. ``tokenized_column: [[string]]``.\n",
        "3. Dictionary estimation.\n",
        "4. Each string in tokenized column is converted into token_id from dictionary. ``text: [[token_id]]``.\n",
        "5. Depending on the parameters CatBoost produce features basing on the resulting text column: Bag of words, Multinomial naive bayes or Bm25.\n",
        "6. Computed float features are passed into the usual CatBoost learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_A87DhGF8SIa",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}